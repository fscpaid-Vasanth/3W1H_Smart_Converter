import fs from "fs";
import path from "path";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

/**
 * ======================================
 * SPEECH ‚Üí TEXT (SAFE + FORMAT FIXED)
 * ======================================
 */
export async function speechToText(filePath) {
  if (!fs.existsSync(filePath)) {
    throw new Error("Audio file not found for STT");
  }

  // üî• FORCE EXTENSION FIX
  let ext = path.extname(filePath).toLowerCase();

  // If no extension ‚Üí assume webm
  if (!ext) {
    const newPath = `${filePath}.webm`;
    fs.renameSync(filePath, newPath);
    filePath = newPath;
    ext = ".webm";
  }

  // Allowed formats by OpenAI
  const allowed = [
    ".flac", ".m4a", ".mp3", ".mp4",
    ".mpeg", ".mpga", ".oga", ".ogg",
    ".wav", ".webm"
  ];

  if (!allowed.includes(ext)) {
    throw new Error(`Unsupported audio format: ${ext}`);
  }

  try {
    const stats = fs.statSync(filePath);
    console.log("üìä STT FILE SIZE:", stats.size, "bytes");
  } catch (e) {
    console.warn("‚ö†Ô∏è Could not get file stats:", e.message);
  }

  console.log("üéß STT FILE:", filePath);

  // üî• SEND WITH CORRECT FILENAME
  const transcription = await openai.audio.transcriptions.create({
    file: fs.createReadStream(filePath),
    model: "whisper-1",
    response_format: "text",
    prompt: "This is a business/industrial analysis recording for the 3W1H framework (What, Who, Where, When, How)."
  });

  console.log("üìù TRANSCRIPTION RESULT:", transcription);

  if (!transcription || !transcription.trim()) {
    throw new Error("Empty transcription from audio");
  }

  return transcription;
}
